---
title: Identifying Files
layout: post
tags: [trick, tutorial]
uuid: be4f6a39-11d0-3963-8ade-1ff1bbf4d904
---

<!-- 20 May 2010 -->
<p>
At work I currently spend about a third of my time
doing <a href="http://en.wikipedia.org/wiki/Data_reduction"> data
reduction</a>, and it's become one of my favorite tasks.
(I've <a href="https://github.com/skeeto/binitools"> done it on my
own</a> too). Data come in from various organizations and sponsors in
all sorts of strange formats. We have a bunch of fancy analysis tools
to work on the data, but they aren't any good if they can't read the
format. So I'm tasked with writing tools to convert incoming data into
a more useful format.
</p>
<p>
If the source file is a text-based file it's usually just a matter of
writing a parser — possibly including a grammar — after carefully
studying the textual structure. Binary files are trickier.
Fortunately, there are a few tools that come in handy for identifying
the format of a strange binary file.
</p>
<p>
The first is the standard utility found on any unix-like
system: <a href="http://packages.debian.org/sid/file"> <code>file</code>
</a>. I have no idea if it has an official website because it's a term
that's impossible to search for. It tries to identify a file based on
the magic numbers and other tests, none based on the actual
file name. I've never been to lucky to have <code>file</code> recognize
a strange format at work. But silence speaks volumes: it means the
data are not packed into something common, like a simple zip archive.
</p>
<p>
Next, I take a look at the file
with <a href="http://www.fourmilab.ch/random/">ent</a>, a pseudo-random
number sequence test program. This will reveal how compressed (or even
encrypted) data are. If ent says the data are very dense, say 7 bits
per byte or more, the format is employing a good compression
algorithm. The next step would be tackling that so I can start over on
the uncompressed contents. If it's something like 4 bits per byte
there's no compression. If it's in between then it might be employing
a weak, custom compression algorithm. I've always seen the latter two.
</p>
<p>
Next I dive in with a hex editor. I use a combination of
Emacs' <code>hexl-mode</code> and the standard BSD
tool <a href="http://code.google.com/p/hexdump/"> hexdump</a> (for
something more static). One of the first things I like to identify is
byte order, and in a hex dump it's often obvious.
</p>
<p>
In general, better designed formats use big endian, also known as
network order. That's the standard ordering used in communication,
regardless of the native byte ordering of the network clients. The
amateur, home-brew formats are generally less thoughtful and dump out
whatever the native format is, usually little endian because that's
what x86 is. Worse, they'll also generate data on architectures that
are big endian, so you can get it both ways without any warning. In
that case your conversion tool has to be sensitive to byte order and
find some way to identify which ordering a file is using. A time-stamp
field is very useful here, because a 64-bit time-stamp read with the
wrong byte order will give a very unreasonable date.
</p>
<p>
For example, here's something I see often.
</p>
<pre>
eb 03 00 00 35 00 00 00 66 1e 00 00
</pre>
<p>
That's most likely 3 4-byte values, in little endian byte order. The
zeros make the integers stand out.
</p>
<pre>
eb 03 00 00 <b>35 00 00 00</b> 66 1e 00 00
</pre>
<p>
We can tell it's little endian because the non-zero digits are on the
left. This information will be useful in identifying more bytes in the
file.
</p>
<p>
Next I'd look for headers, common strings of bytes, so that I can
identify larger structures in the data. I've never had to reverse
engineer a format ... yet. I'm not sure if I could. Once I got this
far I've always been able to research the format further and find
either source code or documentation, revealing everything to me.
</p>
<p>
If the file contains strings I'll dump them out
with <a href="http://en.wikipedia.org/wiki/Strings_(Unix)">
<code>strings</code></a>. I haven't found this too useful at work, but
<a href="/blog/2009/04/18/">it's been useful at home</a>.
</p>
<p>
And there's something still useful beyond these. Something I made
myself at home for a completely different purpose, but I've exploited
its side effects: my <a href="https://github.com/skeeto/pngarch"> PNG
Archiver</a>. The original purpose of the tool is to store a file in
an image, as images are easier to share with others. The side effect
is that by viewing the image I get to see the structure of the
file. For example, here's my laptop's <code>/bin/ls</code>, very
roughly labeled.
</p>
<p class="center">
<img src="/img/pngarch/bin-ls.png" alt="The different segments of the ELF binary are easily visible."/>
</p>
<p>
It's easy to spot the different segments of the ELF format. Higher
entropy sections are more brightly colored. Strings, being composed of
ASCII-like text, have their MSB's unset, which is why they're
darker. Any non-compressed format will have an interesting profile
like this. Here's a Word doc, an infamously horrible format,
</p>
<p class="center">
  <img src="/img/pngarch/word-doc.png" alt=""/>
</p>
<p>
And here's some Emacs bytecode. You can tell the code vectors apart
from the constants section below it.
</p>
<p class="center">
  <img src="/img/pngarch/elc.png" alt=""/>
</p>
<p>
If you find yourself having to inspect strange files, keep these tools
around to make the job easier.
</p>
